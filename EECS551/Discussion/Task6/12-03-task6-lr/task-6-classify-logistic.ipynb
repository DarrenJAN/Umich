{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore logistic regression classification of 5,8 digits   \n",
    "2017-11-27 Jeff Fessler, University of Michigan <br>\n",
    "2018-11-27 Steven Whitaker - Convert to Julia v1.0 <br>\n",
    "2019-11-25 Caroline Crockett - Add comments and sections, function template for NGD <br>\n",
    "2020-11-19 Caroline Crockett - Fix NGD template and change order of the parts <br>\n",
    "2021-11-23 Zongyu Li - Add `MLDatasets` and switch to Julia v1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Formation and General Instructions\n",
    "You may work individually, but we recommend that you work in pairs or groups of three. Find someone to work with and introduce yourself to them! One of you should copy the Google Document template and share the document with edit permissions with your group member(s): \n",
    "https://docs.google.com/document/d/1jw2209GnhUDYXsnoKfolb-HPoznJleDtFedsk-jb_hA/template/preview (you must use your umich email address to access this google document). \n",
    "\n",
    "\n",
    "The Google Document will include your answers to a couple of questions that will be asked. For each question, pick someone to type up your **group's** response. After completing the response, the group member should sign their name by typing it below the answer. For the next question, have a different group member respond and sign their name in the same way. Keep rotating until all the questions have been answered.\n",
    "\n",
    "The goal of this group exercise is to formulate your response as a group to the problem. When finished, **one** student in the group must submit a PDF of the google document to gradescope, entering the uniquenames of **all** students in the group. One group submits exactly one PDF, but we expect the PDF from different groups to differ. The deadline for submitting to gradescope will be announced on Canvas. Only submit the filled-in google document; do not submit a download of this Jupyter notebook. \n",
    "\n",
    "### Overview\n",
    "This task sheet develops logisitic regression based classification for classifying handwritten digits and then compares all the classification methods we have looked at this semester: nearest-angle-based classification (task 1), least-squares based classification (task 2), subspace-based classification (task 3 and 5), and logistic regression classification. \n",
    "\n",
    "We focus on classifying digits “5” and “8” because those are more difficult to distinguish and benefit from more advanced classification methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots: plot, plot!, scatter, scatter!, savefig, title!, histogram, histogram!\n",
    "using MIRTjim: jim\n",
    "using MLDatasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Download and shape data\n",
    "- x0, train0, and test0 contain data for digit \"5\"\n",
    "- x1, train1, and test1 contain data for digit \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(data) = (28, 27, 1000, 2)\n",
      "size(x0) = (28, 27, 1000)\n",
      "size(x1) = (28, 27, 1000)\n"
     ]
    }
   ],
   "source": [
    "# read the MNIST data file for digits 5 and 8\n",
    "# download from web if needed\n",
    "\n",
    "if !@isdefined(data)\n",
    "    digitn = [5, 8]\n",
    "    tmpx, tmpy = MNIST.traindata(Float32)\n",
    "    data = n -> tmpx[:,:,findall(tmpy .== n)[1:1000]] # 1st 1000 of digit n\n",
    "    data = 255 * cat(dims=4, data.(digitn)...)\n",
    "    nx, ny, nrep, ndigit = size(data)\n",
    "    data = data[:,2:ny,:,:] # make images non-square to force debug\n",
    "    ny = size(data,2)\n",
    "    @show size(data)\n",
    "end\n",
    "\n",
    "x0 = data[:,:,:,1]\n",
    "x1 = data[:,:,:,2]\n",
    "\n",
    "@show size(x0);\n",
    "@show size(x1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip910\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip911\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M139.793 1408.41 L2112.76 1408.41 L2112.76 125.286 L139.793 125.286  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip912\">\n",
       "    <rect x=\"139\" y=\"125\" width=\"1974\" height=\"1284\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  521.029,125.286 521.029,1408.41 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1104.66,125.286 1104.66,1408.41 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  139.793,1408.41 2112.76,1408.41 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  521.029,1408.41 521.029,1391.56 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1104.66,1408.41 1104.66,1391.56 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M511.411 1465.14 L519.05 1465.14 L519.05 1438.78 L510.74 1440.44 L510.74 1436.18 L519.004 1434.52 L523.68 1434.52 L523.68 1465.14 L531.319 1465.14 L531.319 1469.08 L511.411 1469.08 L511.411 1465.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M1083.48 1465.14 L1099.8 1465.14 L1099.8 1469.08 L1077.85 1469.08 L1077.85 1465.14 Q1080.52 1462.39 1085.1 1457.76 Q1089.7 1453.11 1090.89 1451.76 Q1093.13 1449.24 1094.01 1447.5 Q1094.91 1445.74 1094.91 1444.05 Q1094.91 1441.3 1092.97 1439.56 Q1091.05 1437.83 1087.95 1437.83 Q1085.75 1437.83 1083.29 1438.59 Q1080.86 1439.36 1078.08 1440.91 L1078.08 1436.18 Q1080.91 1435.05 1083.36 1434.47 Q1085.82 1433.89 1087.85 1433.89 Q1093.22 1433.89 1096.42 1436.58 Q1099.61 1439.26 1099.61 1443.75 Q1099.61 1445.88 1098.8 1447.8 Q1098.01 1449.7 1095.91 1452.3 Q1095.33 1452.97 1092.23 1456.18 Q1089.13 1459.38 1083.48 1465.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M1119.61 1452.67 Q1116.28 1452.67 1114.36 1454.45 Q1112.46 1456.23 1112.46 1459.36 Q1112.46 1462.48 1114.36 1464.26 Q1116.28 1466.05 1119.61 1466.05 Q1122.95 1466.05 1124.87 1464.26 Q1126.79 1462.46 1126.79 1459.36 Q1126.79 1456.23 1124.87 1454.45 Q1122.97 1452.67 1119.61 1452.67 M1114.94 1450.68 Q1111.93 1449.93 1110.24 1447.87 Q1108.57 1445.81 1108.57 1442.85 Q1108.57 1438.71 1111.51 1436.3 Q1114.47 1433.89 1119.61 1433.89 Q1124.77 1433.89 1127.71 1436.3 Q1130.65 1438.71 1130.65 1442.85 Q1130.65 1445.81 1128.96 1447.87 Q1127.3 1449.93 1124.31 1450.68 Q1127.69 1451.46 1129.57 1453.75 Q1131.46 1456.05 1131.46 1459.36 Q1131.46 1464.38 1128.39 1467.06 Q1125.33 1469.75 1119.61 1469.75 Q1113.89 1469.75 1110.82 1467.06 Q1107.76 1464.38 1107.76 1459.36 Q1107.76 1456.05 1109.66 1453.75 Q1111.56 1451.46 1114.94 1450.68 M1113.22 1443.29 Q1113.22 1445.98 1114.89 1447.48 Q1116.58 1448.99 1119.61 1448.99 Q1122.62 1448.99 1124.31 1447.48 Q1126.02 1445.98 1126.02 1443.29 Q1126.02 1440.61 1124.31 1439.1 Q1122.62 1437.6 1119.61 1437.6 Q1116.58 1437.6 1114.89 1439.1 Q1113.22 1440.61 1113.22 1443.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  139.793,183.217 2112.76,183.217 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  139.793,745.23 2112.76,745.23 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  139.793,125.286 139.793,1408.41 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  139.793,183.217 156.641,183.217 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  139.793,745.23 156.641,745.23 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M83.8855 196.562 L91.5243 196.562 L91.5243 170.196 L83.2142 171.863 L83.2142 167.604 L91.478 165.937 L96.1539 165.937 L96.1539 196.562 L103.793 196.562 L103.793 200.497 L83.8855 200.497 L83.8855 196.562 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M56.6171 758.575 L72.9365 758.575 L72.9365 762.51 L50.9921 762.51 L50.9921 758.575 Q53.6541 755.82 58.2375 751.191 Q62.8439 746.538 64.0245 745.195 Q66.2698 742.672 67.1494 740.936 Q68.0522 739.177 68.0522 737.487 Q68.0522 734.732 66.1078 732.996 Q64.1865 731.26 61.0847 731.26 Q58.8856 731.26 56.4319 732.024 Q54.0014 732.788 51.2236 734.339 L51.2236 729.617 Q54.0477 728.482 56.5014 727.904 Q58.955 727.325 60.9921 727.325 Q66.3624 727.325 69.5568 730.01 Q72.7513 732.695 72.7513 737.186 Q72.7513 739.316 71.9411 741.237 Q71.1541 743.135 69.0476 745.728 Q68.4689 746.399 65.367 749.617 Q62.2652 752.811 56.6171 758.575 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M81.5707 727.95 L103.793 727.95 L103.793 729.941 L91.2465 762.51 L86.3623 762.51 L98.1678 731.885 L81.5707 731.885 L81.5707 727.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><g clip-path=\"url(#clip912)\">\n",
       "<image width=\"1232\" height=\"1189\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAABNAAAASlCAYAAABp3SSXAAAgAElEQVR4nOzaW4vd5RnG4f+SmQQN\n",
       "S0xj3Ss60EapGhXSVhIPZFIIilYkxg0qohjFVhqlVKUobhA38UAtIuqJ4Ib2qKESxbqDQNAYjMVI\n",
       "1VYnQrBmQ2ssJqkUXD0oC2woNxjex5XR6/oAN0+GybDWj7fX7/cHHQAAAADwf+0z6gMAAAAAYG8m\n",
       "oAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCg\n",
       "AQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKAB\n",
       "AAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEA\n",
       "AABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAA\n",
       "AEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAA\n",
       "QCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABA\n",
       "IKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAg\n",
       "oAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQDA2\n",
       "6gOAPXfuueeW7N59993Nth577LFmW7t75plnSnbffffdkl0AAACmJy/QAAAAACAQ0AAAAAAgENAA\n",
       "AAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAA\n",
       "AAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAA\n",
       "ACAQ0AAAAAAgGBv1AcCeO+SQQ0p299tvv2Zbv/jFL5pt7e6KK64o2b3kkktKdtetW1eyCwDTxaJF\n",
       "i0p2TzrppJLdhx56qGR3165dJbtQ5cADD2y6N2PGjKZ7Q8uWLSvZnZycLNmdmJhouvfoo4823Ru6\n",
       "7bbbSnanGy/QAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ\n",
       "0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQ\n",
       "AAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBgb9QHA3mf79u3NtjZu3Nhs\n",
       "a3fHH398ye4TTzxRsvvAAw803XvkkUea7g198cUXJbsAcNddd5XsHnrooSW7p512Wslu65/Da6+9\n",
       "1nRv6OCDDy7Z3bJlS8lua1X//iOOOKJkt+vqbr7nnnua7h1wwAFN94Z6vV7J7mAwKNl95513mu49\n",
       "++yzTff4X16gAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAg\n",
       "oAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCg\n",
       "AQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABA0Ov3+4NRHwHsmbvuuqtkd/v27c227rnn\n",
       "nmZbuzv66KNLdn/729+W7B5xxBFN96699tqme0O///3vS3YBmH4OO+ywpnvPP/98072h2bNnl+wO\n",
       "BjVflVp+1uq6rtu4cWPTvaGDDz64ZHfLli0lu60ddNBBJbt/+tOfSna7ru539g9/+EPTvenyO1Dt\n",
       "vffea7q3Y8eOpnv8Ly/QAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQ\n",
       "AAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAA\n",
       "AAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBgb9QHAnjvxxBNL\n",
       "dlevXl2y29qHH35Ysnv++eeX7M6fP7/p3uuvv950DwB2N3v27KZ7M2bMaLpX7cUXXyzZ3bhxY8lu\n",
       "a71er2R3MBiU7HZd151zzjnNtl544YVmW192ww03lOwCtbxAAwAAAIBAQAMAAACAQEADAAAAgEBA\n",
       "AwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEAD\n",
       "AAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMA\n",
       "AACAoNfv9wejPgK+yQ488MCy7dWrV5fsXnnllc221qxZ02wLAPj/rr322pLdG264oWS3tc2bN5fs\n",
       "Ll26tGR3amqqZBeAOl6gAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCg\n",
       "AQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKAB\n",
       "AAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAMDbqA+CbbubMmWXbn332Wcnu\n",
       "hx9+2Gzr8MMPb7a1u5NPPrlkd+3atSW727ZtK9kFYPqYPXt2ye6ll15asjsYDJrubd26tene0NKl\n",
       "S0t2p6amSnYBmH68QAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMA\n",
       "AACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAA\n",
       "AIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBgbNQHwDfdggULyrYP\n",
       "O+ywkt21a9eW7E4Xu3btKtlduXJl070VK1Y03RvaunVryS4AXfeDH/ygZPeQQw4p2f3444+b7l18\n",
       "8cVN94ampqZKdgFgyAs0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0\n",
       "AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQA\n",
       "AAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIxkZ9AHzTjY+Pj/qEr2z79u3N\n",
       "ttauXdtsa3ebN28u2T3nnHNKdi+88MKme6effnrTvaFrrrmmZPf1118v2QWYTiYnJ0d9wlfyxhtv\n",
       "NN37y1/+0nQPAL4uXqABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKAB\n",
       "AAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEA\n",
       "AABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABA0Ov3+4NRHwHfZOPj\n",
       "42XbCxcuLNl9++23m21t27at2dbX5bvf/W7J7llnndV07/rrr2+6N/Txxx+X7J599tklu7t27SrZ\n",
       "Bajwy1/+smR3+fLlJbutbdiwoWT3pZdeKtl9+umnS3ani7/97W+jPgFgr+EFGgAAAAAEAhoAAAAA\n",
       "BAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAE\n",
       "AhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQC\n",
       "GgAAAAAEAhoAAAAABL1+vz8Y9REAfHUXXXRRye69995bsrtkyZKS3ddee61kF6DCvvvuW7L74IMP\n",
       "luwuXry4ZLe1Xq9XsjsYfLu/Kj377LMluy+++GLJbtd13dTUVLOtrVu3Ntv6sk2bNpXsArW8QAMA\n",
       "AACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAA\n",
       "AIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAA\n",
       "gEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgKDX7/cHoz4CgK9u0aJF\n",
       "JbuPP/54ye6pp55asrtp06aSXYDpZNasWSW7c+fOLdltbdmyZSW7BxxwQMnuggULSnZb6/V6JbuD\n",
       "wfT4Crpt27aS3Y8++qhkt+u67u677y7ZXbNmTckuTCdeoAEAAABAIKABAAAAQCCgAQAAAEAgoAEA\n",
       "AABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAA\n",
       "AEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAA\n",
       "QNDr9/uDUR8BwFd3//33l+wuWbKkZHfevHklu3//+99LdgFgxowZJbtz5sxpunfCCSc03Rs69dRT\n",
       "S3YHg7qvoBMTE822Jicnm219Wa/XK9ntuq775JNPSnbXrVtXstva5ZdfPuoT+AbzAg0AAAAAAgEN\n",
       "AAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0A\n",
       "AAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAA\n",
       "AAACAQ0AAAAAAgENAAAAAAIBDQAAAACCXr/fH4z6CIBvg6VLlzbdu++++5ruDW3atKlkd3JysmT3\n",
       "X//6V8kuAECFZcuWlW0vWrSoZHf+/PlN98bHx5vuDa1du7Zk9+abby7Z/fOf/1yySw0v0AAAAAAg\n",
       "ENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ\n",
       "0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQ\n",
       "AAAAACAQ0AAAAAAgENAAAAAAIOj1+/3BqI+APTE+Pt507+qrr266N/TUU0+V7HZd1/3jH/8o2/42\n",
       "O+qoo0p277///qZ7P/zhD5vuDd15550luw8//HDJLgAAtRYsWNB07/bbb2+6NzR37tyS3eeee65k\n",
       "d/ny5U33duzY0XSP/+UFGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQC\n",
       "GgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIa\n",
       "AAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABGOjPgD21JIlS5ruzZ8/v+ne\n",
       "0G9+85uSXbruhBNOKNl98sknS3bnzJnTdG/9+vVN94ZeeeWVkl0AqPLMM8+U7G7evLlk91e/+lXT\n",
       "vU8++aTpHuxuzZo1Tfd+97vfNd0buuWWW0p2Fy9eXLI7d+7cpntV3w/4Ly/QAAAAACAQ0AAAAAAg\n",
       "ENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ\n",
       "0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQ\n",
       "AAAAACAQ0AAAAAAgENAAAAAAIBgb9QGwpz799NOmezt37my6x3/95Cc/Kdu+7777SnbnzJlTsrt+\n",
       "/fqme5dccknTvaHW/7cAoNrYWM3XmsWLF5fstv6sUfWZYMeOHSW7TD8TExNN96688sqme9Wefvrp\n",
       "kt0NGzaU7FLDCzQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAA\n",
       "CAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAI\n",
       "BDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAh6/X5/MOojYE/MnDmz6d4f//jHpntD\n",
       "L7/8cslu13XdmjVrSnYvu+yyZlsLFy5strW7sbGxkt0PPvigZHf58uVN9958882mewAwXc2aNatk\n",
       "d+XKlSW7xx13XNO9DRs2NN0bavmZ8Mu2bNlSskvXHXvssSW7N910U9O9ycnJpntDn332Wcnuz372\n",
       "s5Ldl156qWSXGl6gAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAA\n",
       "AEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAA\n",
       "QCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQNDr9/uDUR8Be4Obb765\n",
       "ZPeqq64q2aXr3n///ZLd66+/vmR3/fr1JbsAQI0zzjijZPfGG29sunfMMcc03Rv6/PPPS3Zvuumm\n",
       "kt2u67qpqamy7VaWLVtWtn3KKaeU7M6cObPp3saNG5vuDV133XUlu9Ph94p6XqABAAAAQCCgAQAA\n",
       "AEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAA\n",
       "QCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABA\n",
       "IKABAAAAQCCgAQAAAEDQ6/f7g1EfAXuDffap6ckLFy4s2e26rjvzzDNLdo877rhmW6tXr262tbtH\n",
       "H320ZPef//xnyS4AQNd13ezZs5vu/frXv266N3TBBReU7A4G3+6voL1er2y76mf74IMPNt1bsWJF\n",
       "0z34OniBBgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYA\n",
       "AAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAA\n",
       "AACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAEGv3+8PRn0EAAAAe5dzzz23ZPf4\n",
       "448v2e26rpuYmGi2tWXLlmZbX7Zz586S3a7ruhdeeKFkd926dU33/v3vfzfdg6+DF2gAAAAAEAho\n",
       "AAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgA\n",
       "AAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAA\n",
       "AAAQCGgAAAAAEAhoAAAAABD0+v3+YNRHAAAAAMDeygs0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAA\n",
       "AAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAA\n",
       "CAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAI\n",
       "BDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgE\n",
       "NAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0\n",
       "AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQA\n",
       "AAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAA\n",
       "AAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAA\n",
       "AAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAA\n",
       "CAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAI\n",
       "BDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgE\n",
       "NAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0\n",
       "AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQA\n",
       "AAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAA\n",
       "AAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAA\n",
       "AAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAA\n",
       "CAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAI\n",
       "BDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgE\n",
       "NAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0\n",
       "AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQA\n",
       "AAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAA\n",
       "AAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAjGRn0AsOeOPvrokt1jjz222daR\n",
       "Rx7ZbGt3J598csnuoYceWrI7Pj7edG/9+vVN94buvPPOkt3PP/+8ZBcAvu3mzZtXsvvTn/60ZPfH\n",
       "P/5xyW7Xdd2JJ55Ytj0d/PWvfy3ZPf3000t2YTrxAg0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAA\n",
       "AgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAAC\n",
       "AQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIB\n",
       "DQAAAACCsVEfAHuLWbNmlezeeuutJbtd13XnnXdeye7YmD8N08FJJ51Usvud73ynZPfnP/95yS4A\n",
       "TDdnnnlm070VK1Y03Rvaf//9S3YHg0HJbtd13VtvvdVs69VXX2229WXz5s0r2e26rvvRj35Ustv6\n",
       "d3bVqlVN9+Dr4AUaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAA\n",
       "AAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAA\n",
       "BAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEvX6/Pxj1EbAnjjzyyKZ7Dz30UNO9\n",
       "oVNOOaVklzrvv/9+ye5g0PbP7cTERNO9agsXLizZ3bRpU8kuAOy3334lu6tWrWq6973vfa/p3lDV\n",
       "5+MHHnigZLfrum7nzp1l2618//vfL9teuXJlye7+++/fdO+RRx5pujd0xx13lOxC13mBBgAAAACR\n",
       "gAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABv9p125etK7bPo7/Thu6RmssHKgW\n",
       "QQMxLSLoiRg3oiQYaYugTZIJkWiUaAs3FoItRCJaBAnZsghEdFH2ILMIIhdDYtDWBEWDUBp7WIyW\n",
       "Mee1uBnwdvHhKr5HZ6e+Xn/Ah2MxDL95zxcAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAA\n",
       "IBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAg\n",
       "ENAAAAAAIBDQAAAAACAYGfQB8Hft2bOn6d4jjzzSdI//s3fv3rLtQ4cOlez++uuvJbutPffccyW7\n",
       "b7zxRsnusmXLSnbPnTtXsgsA27dvL9mdnJxsujczM9N0b0Hld9yN7IcffijbvnTpUsnu0qVLm+59\n",
       "//33Tffgn+AFGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAE\n",
       "AhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQC\n",
       "GgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABCODPoDr30MPPVSyu3LlypJduu7SpUvN\n",
       "to4cOdJs61rnz58v2x4GH330Ucnutm3bSnafeuqpkt3vvvuuZBcA1q5dO+gT/ic7duwY9An8Bbt2\n",
       "7SrbvuOOO0p2T58+3XTvwIEDTffgn+AFGpdpTz4AABDYSURBVAAAAAAEAhoAAAAABAIaAAAAAAQC\n",
       "GgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIa\n",
       "AAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoA\n",
       "AAAABAIaAAAAAAQjgz6A69/o6GjJ7k033VSyO0wuXLhQsvvyyy832zp79myzLf6/8fHxkt3//Oc/\n",
       "JbuPP/54ye6ePXtKdgEYHkuWLBmqXYbDxo0bS3aff/75kt2u67p+v1+ye/HixZJdGCZeoAEAAABA\n",
       "IKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAg\n",
       "oAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCg\n",
       "AQAAAEAgoAEAAABAIKABAAAAQDAy6AO4/v3xxx8lu/Pz8033Fi0avp78xRdflOzOzMyU7A6Lu+66\n",
       "a9An/E92795dsnvrrbeW7F6+fLlkFwAWL148VLu9Xq9kl65bsWJFs61du3Y12/qnHD58uGT3wIED\n",
       "JbswTIavGAAAAADAP0hAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBA\n",
       "AwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEAD\n",
       "AAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBgZNAHcP379ttvS3aPHz/edG9qaqrp3j9h\n",
       "/fr1Jbv33ntvs63p6elmW9dat25dye4w/iwMg1OnTpXsLlrU9n9B8/PzTfcAqDc7O1uyOzMzU7K7\n",
       "Zs2apntV30T79u0r2R0fHy/Z7bque+utt5ptjY6ONtu62v79+0t2u67r9u7dW7L7559/luzCMPEC\n",
       "DQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgEN\n",
       "AAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0A\n",
       "AAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAILe2NhYf9BHwN/xwgsvNN3bsmVL070Fp0+f\n",
       "Ltntuq5bsWJF2faNrNfrlez2+37dVti0aVPTvaNHjzbdA2B4LV++vGT38OHDTfeqvjHWrFlTsvva\n",
       "a6+V7HZd161atarZ1szMTLOtq1X93dF1XTc7O1u2DTc6L9AAAAAAIBDQAAAAACAQ0AAAAAAgENAA\n",
       "AAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAA\n",
       "AAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAA\n",
       "ACDojY2N9Qd9BPwdt912W9O9J554ounegk8//bRkt+u6bnp6umR3YmKiZJe2Pv/885LdW265pWR3\n",
       "5cqVJbs//fRT071Vq1Y13Vvwyy+/lOwCMHzef//9pntPPvlk070Fly9fLtkdHR0t2e26ruv1es22\n",
       "nn322WZbVzt27FjJLlDLCzQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAI\n",
       "BDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgE\n",
       "NAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0\n",
       "AAAAAAh6Y2Nj/UEfAdez5cuXl20fOnSobLuVfr/uV8z09HTJ7pEjR0p2P/nkk6Z78/PzTfeq7dy5\n",
       "s2T3lVdeabq3adOmpnsLjh49WrILwPBZsmRJ072qb6KJiYmS3UoHDhxotrVjx45mW8Dw8wINAAAA\n",
       "AAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAA\n",
       "AgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAAC\n",
       "AQ0AAAAAAgENAAAAAAIBDQAAAACCkUEfANe7LVu2DPqEv2xubq7Z1urVq5ttXevcuXNl27T35ptv\n",
       "luyuWbOm6d7mzZub7i04evRoyS4Aw6flt1bX1X0TTUxMlOz2+/2S3a7rupmZmbJt4MbmBRoAAAAA\n",
       "BAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAE\n",
       "AhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQC\n",
       "GgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEI4M+AP4t7rvvvpLd1atXl+xWOn78eLOtc+fONdti\n",
       "uM3Pz5fs9vv9pnsPPPBA070FExMTJbtnzpwp2QWgztatW5vurVixouletV6vV7a9fv36ZlvT09PN\n",
       "tq7222+/lewCtbxAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAA\n",
       "AIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAA\n",
       "gEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAYGTQB8C/Ra/XK9ldtKiuU1+5cqVk\n",
       "9+DBgyW7MAyWLFlSsnvnnXeW7J45c6ZkF4CuGx8fL9ndsGFD071+v990b8GLL75Ysvvuu++W7HZd\n",
       "101NTTXbmpycbLZ1tRMnTpTsArW8QAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBA\n",
       "AwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEAD\n",
       "AAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBAQAMAAACAQEADAAAAgEBAAwAAAIBgZNAH\n",
       "AH/f7Oxsye7HH39csgsAMEx27txZsnv33Xc33Tt06FDTvQXT09Mlu2+//XbJbtd13a5du5ptPfjg\n",
       "g822rnbixImSXaCWF2gAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgA\n",
       "AAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAA\n",
       "AAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABCMDPoAAG4MzzzzTMnuPffcU7IL\n",
       "wPAYHx8v2V27dm3J7tzcXNO9vXv3Nt2rdvLkyUGfAPCXeYEGAAAAAIGABgAAAACBgAYAAAAAgYAG\n",
       "AAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYA\n",
       "AAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAAAACBgAYAAAAAgYAGAAAAAIGABgAA\n",
       "AADByKAPgH+Ln3/+uWT34sWLJbtd13Xj4+Mluzt27Gi29eGHHzbbutb58+dLdkdHR0t2b7/99qZ7\n",
       "a9eubbq3YN26dSW7Dz/8cMnuzTff3HRvdna26d6CH3/8sWQXgK57+umnS3aXLl1asnvs2LGme1Xf\n",
       "RFWWL18+6BMA/jIv0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAA\n",
       "AAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAA\n",
       "ACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAYGfQB8G9x4cKFkt2D\n",
       "Bw+W7HZd17300kslu6+++mqzre3btzfbutY333xTsrts2bKS3cnJyZLdG93ly5eb7m3btq3p3oKz\n",
       "Z8+W7ALQdVNTUyW7/X6/ZPeDDz4o2W1ty5YtJbubN28u2e26rpubm2u29eWXXzbbAoafF2gAAAAA\n",
       "EAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQ\n",
       "CGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAIaAAAAAAQCGgAAAAAEAhoAAAAABAI\n",
       "aAAAAAAQCGgAAAAAEAhoAAAAABCMDPoAuN699957ZduPPfZYye6jjz7abKvX6zXbutbU1FTZNu39\n",
       "/vvvJbuvv/56072vvvqq6R4Aw6vqO2br1q1N9955552mewsWL15csjs3N1ey23Vdt3HjxmZbZ86c\n",
       "abYFDD8v0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAA\n",
       "AAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAA\n",
       "AAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACDojY2N9Qd9BPD3jI6Oluzu3r27\n",
       "2daGDRuabQ27I0eONN377LPPmu4tuHLlSsnu119/XbI7NzdXsgvA8Lj//vtLdvft21eyOzk52XTv\n",
       "5MmTTfcWnDp1qmR3//79Jbtd13UnTpwo2wZubF6gAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAg\n",
       "oAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCg\n",
       "AQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABAIKABAAAAQCCgAQAAAEAgoAEAAABA0Bsb\n",
       "G+sP+ggAAAAA+LfyAg0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0A\n",
       "AAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAA\n",
       "AAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAA\n",
       "AAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAA\n",
       "AgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAAC\n",
       "AQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIB\n",
       "DQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgEN\n",
       "AAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0AAAAAAgENAAAAAAIBDQAAAAACAQ0A\n",
       "AAAAAgENAAAAAIL/Ai3T0U3F2kcIAAAAAElFTkSuQmCC\n",
       "\" transform=\"translate(510, 172)\"/>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#ffff00; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  499.413,161.601 1126.27,161.601 1126.27,766.846 499.413,766.846 499.413,161.601 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#ffff00; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1126.27,161.601 1753.14,161.601 1753.14,766.846 1126.27,766.846 1126.27,161.601 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#ffff00; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  499.413,766.846 1126.27,766.846 1126.27,1372.09 499.413,1372.09 499.413,766.846 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#ffff00; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1126.27,766.846 1753.14,766.846 1753.14,1372.09 1126.27,1372.09 1126.27,766.846 \n",
       "  \"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip913\">\n",
       "    <rect x=\"2160\" y=\"125\" width=\"73\" height=\"1284\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip913)\">\n",
       "<image width=\"72\" height=\"1283\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAUDCAYAAABiSmZPAAAKQklEQVR4nO3WsQ2AMBAEQUD0X6wb\n",
       "eCowG0IwUwFanR6fa6052Lq+/oC/EygIFAQKAoV7xk/sjQUFgYJAQaDgSAcLCgIFgYJAwZEOFhQE\n",
       "CgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoCBQECgIFgYJAwUs6\n",
       "WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGC\n",
       "QMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAoCBYGCQEGgIFDwkg4WFAQKAgWBgiMdLCgI\n",
       "FAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBI\n",
       "BwsKAgWBgkDBkQ4WFAQKAgWBgkBBoCBQECgIFLykgwUFgYJAQaDgSAcLCgIFgYJAwZEOFhQECgIF\n",
       "gYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUF\n",
       "gYJAQaAgUBAoCBQECgIFL+lgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGCQMGR\n",
       "DhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKAgUBAoCBYGC\n",
       "QMFLOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsK\n",
       "AgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKAgWBgkBBoCBQ8JIOFhQECgIFgYIj\n",
       "HSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJA\n",
       "QaDgSAcLCgIFgYJAwZEOFhQECgIFgYJAQaAgUBAoCBS8pIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYU\n",
       "BAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBw\n",
       "pIMFBYGCQEGgIFAQKAgUBAoCBS/pYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWB\n",
       "gkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKAgV/sWBBQaAgUBAoONLBgoJAQaAgUBAo\n",
       "CBQECgIFgYKXdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGC\n",
       "QMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUBAoCBYGCQEGg4CUdLCgI\n",
       "FAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBI\n",
       "BwsKAgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgWBgkBBoCBQECh4SQcLCgIFgYJAwZEOFhQECgIF\n",
       "gYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUF\n",
       "gYJAQaDgSAcLCgIFgYJAQaAgUBAoCBQECl7SwYKCQEGgIFBwpIMFBYGCQEGg4EgHCwoCBYGCQMGR\n",
       "DhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGg\n",
       "IFAQKAgUBAoCBYGCl3SwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsK\n",
       "AgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFAQKAgWBgkBBoOAl\n",
       "HSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJA\n",
       "QaDgSAcLCgIFgYJAwZEOFhQECgIFgYIjHSwoCBQECgIFgYJAQaAgUBAoeEkHCwoCBYGCQMGRDhYU\n",
       "BAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBw\n",
       "pIMFBYGCQEGg4EgHCwoCBYGCQEGgIFAQKAgUBApe0sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsKAgWB\n",
       "gkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGC\n",
       "gkBBoCBQECgIFAQKAgWBgpd0sKAgUBAoCBQc6WBBQaAgUBAoCBT8xYIFBYGCQEGg4EgHCwoCBYGC\n",
       "QMGRDhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKAgUBAoC\n",
       "BYGCQMFLOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBI\n",
       "BwsKAgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKAgWBgkBBoCBQ8JIOFhQECgIF\n",
       "gYIjHSwoCBQECgIFRzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUF\n",
       "gYJAQaDgSAcLCgIFgYJAwZEOFhQECgIFgYJAQaAgUBAoCBS8pIMFBYGCQEGg4EgHCwoCBYGCQMGR\n",
       "DhYUBAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGg\n",
       "IFBwpIMFBYGCQEGgIFAQKAgUBAoCBS/pYEFBoCBQECg40sGCgkBBoCBQcKSDBQWBgkBBoOBIBwsK\n",
       "AgWBgkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECgI\n",
       "FAQKAgWBgkDBSzpYUBAoCBQECo50sKAgUBAoCBQc6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJA\n",
       "QaDgSAcLCgIFgYJAwZEOFhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECgIFgYJAQaAgUPCSDhYU\n",
       "BAoCBYGCIx0sKAgUBAoCBUc6WFAQKAgUBAqOdLCgIFAQKAgUHOlgQUGgIFAQKDjSwYKCQEGgIFBw\n",
       "pIMFBYGCQEGg4EgHCwoCBYGCQMGRDhYUBAoCBYGCQEGgIFAQKAgUvKSDBQWBgkBBoOBIBwsKAgWB\n",
       "gkDBkQ4WFAQKAgWBgiMdLCgIFAQKAgVHOlhQECgIFAQKjnSwoCBQECgIFBzpYEFBoCBQECg40sGC\n",
       "gkBBoCBQcKSDBQWBgkBBoCBQECgIFAQKAgUv6WBBQaAgUBAoONLBgoJAQaAgUHCkgwUFgYJAQaDg\n",
       "SAcLCgIFgYJAwZEOFhQECgIFgYIjHSwoCBQECgIFRzpYUBAoCBQECg9SZb8TM1V3fgAAAABJRU5E\n",
       "rkJggg==\n",
       "\" transform=\"translate(2161, 125)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M2280.7 1390.58 Q2277.09 1390.58 2275.26 1394.14 Q2273.45 1397.68 2273.45 1404.81 Q2273.45 1411.92 2275.26 1415.48 Q2277.09 1419.02 2280.7 1419.02 Q2284.33 1419.02 2286.14 1415.48 Q2287.97 1411.92 2287.97 1404.81 Q2287.97 1397.68 2286.14 1394.14 Q2284.33 1390.58 2280.7 1390.58 M2280.7 1386.87 Q2286.51 1386.87 2289.57 1391.48 Q2292.64 1396.06 2292.64 1404.81 Q2292.64 1413.54 2289.57 1418.14 Q2286.51 1422.73 2280.7 1422.73 Q2274.89 1422.73 2271.81 1418.14 Q2268.76 1413.54 2268.76 1404.81 Q2268.76 1396.06 2271.81 1391.48 Q2274.89 1386.87 2280.7 1386.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2270.21 1135.9 L2288.57 1135.9 L2288.57 1139.84 L2274.5 1139.84 L2274.5 1148.31 Q2275.52 1147.96 2276.53 1147.8 Q2277.55 1147.62 2278.57 1147.62 Q2284.36 1147.62 2287.74 1150.79 Q2291.12 1153.96 2291.12 1159.38 Q2291.12 1164.96 2287.64 1168.06 Q2284.17 1171.14 2277.85 1171.14 Q2275.68 1171.14 2273.41 1170.77 Q2271.16 1170.4 2268.76 1169.65 L2268.76 1164.96 Q2270.84 1166.09 2273.06 1166.65 Q2275.28 1167.2 2277.76 1167.2 Q2281.77 1167.2 2284.1 1165.09 Q2286.44 1162.99 2286.44 1159.38 Q2286.44 1155.77 2284.1 1153.66 Q2281.77 1151.55 2277.76 1151.55 Q2275.89 1151.55 2274.01 1151.97 Q2272.16 1152.39 2270.21 1153.27 L2270.21 1135.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2310.33 1138.98 Q2306.72 1138.98 2304.89 1142.55 Q2303.08 1146.09 2303.08 1153.22 Q2303.08 1160.33 2304.89 1163.89 Q2306.72 1167.43 2310.33 1167.43 Q2313.96 1167.43 2315.77 1163.89 Q2317.6 1160.33 2317.6 1153.22 Q2317.6 1146.09 2315.77 1142.55 Q2313.96 1138.98 2310.33 1138.98 M2310.33 1135.28 Q2316.14 1135.28 2319.2 1139.89 Q2322.27 1144.47 2322.27 1153.22 Q2322.27 1161.95 2319.2 1166.55 Q2316.14 1171.14 2310.33 1171.14 Q2304.52 1171.14 2301.44 1166.55 Q2298.39 1161.95 2298.39 1153.22 Q2298.39 1144.47 2301.44 1139.89 Q2304.52 1135.28 2310.33 1135.28 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2269.43 914.938 L2277.07 914.938 L2277.07 888.572 L2268.76 890.239 L2268.76 885.979 L2277.02 884.313 L2281.7 884.313 L2281.7 914.938 L2289.33 914.938 L2289.33 918.873 L2269.43 918.873 L2269.43 914.938 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2308.78 887.391 Q2305.17 887.391 2303.34 890.956 Q2301.53 894.498 2301.53 901.628 Q2301.53 908.734 2303.34 912.299 Q2305.17 915.84 2308.78 915.84 Q2312.41 915.84 2314.22 912.299 Q2316.05 908.734 2316.05 901.628 Q2316.05 894.498 2314.22 890.956 Q2312.41 887.391 2308.78 887.391 M2308.78 883.688 Q2314.59 883.688 2317.64 888.294 Q2320.72 892.878 2320.72 901.628 Q2320.72 910.354 2317.64 914.961 Q2314.59 919.544 2308.78 919.544 Q2302.97 919.544 2299.89 914.961 Q2296.83 910.354 2296.83 901.628 Q2296.83 892.878 2299.89 888.294 Q2302.97 883.688 2308.78 883.688 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2338.94 887.391 Q2335.33 887.391 2333.5 890.956 Q2331.7 894.498 2331.7 901.628 Q2331.7 908.734 2333.5 912.299 Q2335.33 915.84 2338.94 915.84 Q2342.57 915.84 2344.38 912.299 Q2346.21 908.734 2346.21 901.628 Q2346.21 894.498 2344.38 890.956 Q2342.57 887.391 2338.94 887.391 M2338.94 883.688 Q2344.75 883.688 2347.81 888.294 Q2350.89 892.878 2350.89 901.628 Q2350.89 910.354 2347.81 914.961 Q2344.75 919.544 2338.94 919.544 Q2333.13 919.544 2330.05 914.961 Q2327 910.354 2327 901.628 Q2327 892.878 2330.05 888.294 Q2333.13 883.688 2338.94 883.688 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2269.43 663.346 L2277.07 663.346 L2277.07 636.98 L2268.76 638.647 L2268.76 634.387 L2277.02 632.721 L2281.7 632.721 L2281.7 663.346 L2289.33 663.346 L2289.33 667.281 L2269.43 667.281 L2269.43 663.346 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2298.83 632.721 L2317.18 632.721 L2317.18 636.656 L2303.11 636.656 L2303.11 645.128 Q2304.13 644.781 2305.14 644.619 Q2306.16 644.434 2307.18 644.434 Q2312.97 644.434 2316.35 647.605 Q2319.73 650.776 2319.73 656.193 Q2319.73 661.772 2316.26 664.873 Q2312.78 667.952 2306.46 667.952 Q2304.29 667.952 2302.02 667.582 Q2299.77 667.211 2297.37 666.471 L2297.37 661.772 Q2299.45 662.906 2301.67 663.461 Q2303.89 664.017 2306.37 664.017 Q2310.38 664.017 2312.71 661.91 Q2315.05 659.804 2315.05 656.193 Q2315.05 652.582 2312.71 650.475 Q2310.38 648.369 2306.37 648.369 Q2304.5 648.369 2302.62 648.785 Q2300.77 649.202 2298.83 650.082 L2298.83 632.721 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2338.94 635.799 Q2335.33 635.799 2333.5 639.364 Q2331.7 642.906 2331.7 650.035 Q2331.7 657.142 2333.5 660.707 Q2335.33 664.248 2338.94 664.248 Q2342.57 664.248 2344.38 660.707 Q2346.21 657.142 2346.21 650.035 Q2346.21 642.906 2344.38 639.364 Q2342.57 635.799 2338.94 635.799 M2338.94 632.096 Q2344.75 632.096 2347.81 636.702 Q2350.89 641.286 2350.89 650.035 Q2350.89 658.762 2347.81 663.369 Q2344.75 667.952 2338.94 667.952 Q2333.13 667.952 2330.05 663.369 Q2327 658.762 2327 650.035 Q2327 641.286 2330.05 636.702 Q2333.13 632.096 2338.94 632.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2274.38 411.754 L2290.7 411.754 L2290.7 415.689 L2268.76 415.689 L2268.76 411.754 Q2271.42 408.999 2276 404.369 Q2280.61 399.717 2281.79 398.374 Q2284.03 395.851 2284.91 394.115 Q2285.82 392.356 2285.82 390.666 Q2285.82 387.911 2283.87 386.175 Q2281.95 384.439 2278.85 384.439 Q2276.65 384.439 2274.2 385.203 Q2271.77 385.967 2268.99 387.518 L2268.99 382.795 Q2271.81 381.661 2274.27 381.082 Q2276.72 380.504 2278.76 380.504 Q2284.13 380.504 2287.32 383.189 Q2290.52 385.874 2290.52 390.365 Q2290.52 392.494 2289.7 394.416 Q2288.92 396.314 2286.81 398.906 Q2286.23 399.578 2283.13 402.795 Q2280.03 405.99 2274.38 411.754 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2310.51 384.207 Q2306.9 384.207 2305.08 387.772 Q2303.27 391.314 2303.27 398.443 Q2303.27 405.55 2305.08 409.115 Q2306.9 412.656 2310.51 412.656 Q2314.15 412.656 2315.95 409.115 Q2317.78 405.55 2317.78 398.443 Q2317.78 391.314 2315.95 387.772 Q2314.15 384.207 2310.51 384.207 M2310.51 380.504 Q2316.33 380.504 2319.38 385.11 Q2322.46 389.693 2322.46 398.443 Q2322.46 407.17 2319.38 411.777 Q2316.33 416.36 2310.51 416.36 Q2304.7 416.36 2301.63 411.777 Q2298.57 407.17 2298.57 398.443 Q2298.57 389.693 2301.63 385.11 Q2304.7 380.504 2310.51 380.504 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2340.68 384.207 Q2337.07 384.207 2335.24 387.772 Q2333.43 391.314 2333.43 398.443 Q2333.43 405.55 2335.24 409.115 Q2337.07 412.656 2340.68 412.656 Q2344.31 412.656 2346.12 409.115 Q2347.95 405.55 2347.95 398.443 Q2347.95 391.314 2346.12 387.772 Q2344.31 384.207 2340.68 384.207 M2340.68 380.504 Q2346.49 380.504 2349.54 385.11 Q2352.62 389.693 2352.62 398.443 Q2352.62 407.17 2349.54 411.777 Q2346.49 416.36 2340.68 416.36 Q2334.87 416.36 2331.79 411.777 Q2328.73 407.17 2328.73 398.443 Q2328.73 389.693 2331.79 385.11 Q2334.87 380.504 2340.68 380.504 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2274.38 160.162 L2290.7 160.162 L2290.7 164.097 L2268.76 164.097 L2268.76 160.162 Q2271.42 157.407 2276 152.777 Q2280.61 148.125 2281.79 146.782 Q2284.03 144.259 2284.91 142.523 Q2285.82 140.763 2285.82 139.074 Q2285.82 136.319 2283.87 134.583 Q2281.95 132.847 2278.85 132.847 Q2276.65 132.847 2274.2 133.611 Q2271.77 134.375 2268.99 135.926 L2268.99 131.203 Q2271.81 130.069 2274.27 129.49 Q2276.72 128.912 2278.76 128.912 Q2284.13 128.912 2287.32 131.597 Q2290.52 134.282 2290.52 138.773 Q2290.52 140.902 2289.7 142.824 Q2288.92 144.722 2286.81 147.314 Q2286.23 147.986 2283.13 151.203 Q2280.03 154.398 2274.38 160.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2300.56 129.537 L2318.92 129.537 L2318.92 133.472 L2304.84 133.472 L2304.84 141.944 Q2305.86 141.597 2306.88 141.435 Q2307.9 141.25 2308.92 141.25 Q2314.7 141.25 2318.08 144.421 Q2321.46 147.592 2321.46 153.009 Q2321.46 158.587 2317.99 161.689 Q2314.52 164.768 2308.2 164.768 Q2306.02 164.768 2303.76 164.398 Q2301.51 164.027 2299.1 163.286 L2299.1 158.587 Q2301.19 159.722 2303.41 160.277 Q2305.63 160.833 2308.11 160.833 Q2312.11 160.833 2314.45 158.726 Q2316.79 156.62 2316.79 153.009 Q2316.79 149.398 2314.45 147.291 Q2312.11 145.185 2308.11 145.185 Q2306.23 145.185 2304.36 145.601 Q2302.51 146.018 2300.56 146.898 L2300.56 129.537 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M2340.68 132.615 Q2337.07 132.615 2335.24 136.18 Q2333.43 139.722 2333.43 146.851 Q2333.43 153.958 2335.24 157.523 Q2337.07 161.064 2340.68 161.064 Q2344.31 161.064 2346.12 157.523 Q2347.95 153.958 2347.95 146.851 Q2347.95 139.722 2346.12 136.18 Q2344.31 132.615 2340.68 132.615 M2340.68 128.912 Q2346.49 128.912 2349.54 133.518 Q2352.62 138.101 2352.62 146.851 Q2352.62 155.578 2349.54 160.185 Q2346.49 164.768 2340.68 164.768 Q2334.87 164.768 2331.79 160.185 Q2328.73 155.578 2328.73 146.851 Q2328.73 138.101 2331.79 133.518 Q2334.87 128.912 2340.68 128.912 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.76,1408.41 2232.76,1408.41 2256.76,1408.41 2232.76,1408.41 2232.76,1156.81 2256.76,1156.81 2232.76,1156.81 2232.76,905.222 2256.76,905.222 2232.76,905.222 \n",
       "  2232.76,653.63 2256.76,653.63 2232.76,653.63 2232.76,402.038 2256.76,402.038 2232.76,402.038 2232.76,150.445 2256.76,150.445 2232.76,150.445 2232.76,125.286 \n",
       "  \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at a couple of the images\n",
    "jim(cat(x0[:,:,44:45], x1[:,:,654:655], dims = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some data for training, and some for test\n",
    "ntrain = 100\n",
    "ntest = nrep - ntrain\n",
    "train0 = x0[:,:,1:ntrain] # training data\n",
    "train1 = x1[:,:,1:ntrain]\n",
    "test0 = x0[:,:,(ntrain+1):end] # testing data\n",
    "test1 = x1[:,:,(ntrain+1):end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 27, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×756 Matrix{Float32}:\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0  …    -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0       -0.0  -0.0  -128.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0       -0.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0   -18.0  -0.0    -0.0       -0.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0  -219.0  -0.0    -0.0       -0.0  -0.0  -128.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0    -0.0  …    -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0   -45.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -186.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -253.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -253.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -198.0  -0.0  -150.0  …    -0.0  -0.0  -128.0  -255.0  -0.0\n",
       " -0.0   -0.0  -182.0  -0.0   -27.0       -0.0  -0.0  -255.0  -255.0  -0.0\n",
       " -0.0   -0.0  -247.0  -0.0    -0.0       -0.0  -0.0  -128.0  -191.0  -0.0\n",
       "  ⋮                                 ⋱                                 ⋮\n",
       " -0.0  -82.0    -0.0  -0.0    -0.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0  -56.0    -0.0  -0.0    -0.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0  -39.0    -0.0  -0.0   -46.0  …  -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -139.0  -0.0  -130.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -183.0     -255.0  -0.0  -191.0    -0.0  -0.0\n",
       " -0.0   -0.0  -190.0  -0.0  -253.0     -255.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0    -2.0  -0.0  -253.0     -255.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0  -207.0  …  -255.0  -0.0   -64.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -2.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0       -0.0  -0.0    -0.0    -0.0  -0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train0\n",
    "a = reshape(train0, 100, 28*27) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "This part is the core part of the logistic regression classifier.\n",
    "+ Following the course notes, form a training data matrix A that has 200 rows (one row per training data sample), where the rows\n",
    "are multiplied by ±1 based on using a class label of -1 for digit 5 and class label +1 for digit 8 \n",
    "+ Normalize each row of A to have unit norm.\n",
    "+ Use the formula in the lecture notes to compute the Lipschitz constant for the logistic regression cost\n",
    "function gradient for β = 0.1. Record the value of L in your report.\n",
    "+ Implement Nesterov’s fast gradient descent (FGD).\n",
    "+ Apply FGD to the logistic regression cost function. In your report, include the plot of the\n",
    "cost function versus iteration and discuss whether FGD has converged. \n",
    "\n",
    "The logistic regression cost function is: <br>\n",
    "$f(x) = \\sum_i h(y_i ⟨x,v_i⟩) + \\frac{β}{2} \\lVert x \\rVert_2^2$ <br>\n",
    "$f(x) = 1_M' h.(Ax)         + \\frac{β}{2} \\lVert x \\rVert_2^2 $\n",
    "\n",
    "Where $h(z) = log(1+e^{-z})$ and $A[i,:] = y_i v_i'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×756 Matrix{Float32}:\n",
       " 0.0    0.0    0.0  0.0    0.0    0.0  …    0.0  0.0    0.0    0.0  254.0\n",
       " 0.0    0.0    0.0  0.0    0.0    0.0       0.0  0.0   43.0    0.0  254.0\n",
       " 0.0    0.0    0.0  0.0    3.0    0.0       0.0  0.0  246.0    0.0  254.0\n",
       " 0.0    0.0    0.0  0.0  171.0    0.0       0.0  0.0  254.0    0.0  136.0\n",
       " 0.0    0.0    0.0  0.0  254.0    0.0       0.0  0.0  159.0    0.0   82.0\n",
       " 0.0    0.0    0.0  0.0  254.0    0.0  …  242.0  0.0    6.0    0.0   15.0\n",
       " 0.0    0.0    0.0  0.0  254.0    0.0     254.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  110.0  0.0  179.0    0.0     215.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  253.0  0.0    0.0    0.0      36.0  0.0  202.0    0.0    0.0\n",
       " 0.0    0.0  253.0  0.0    0.0    0.0       0.0  0.0  254.0    0.0    0.0\n",
       " 0.0    0.0  253.0  0.0    0.0    0.0  …    0.0  0.0   52.0    0.0    0.0\n",
       " 0.0    0.0  246.0  0.0    0.0   64.0       0.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  161.0  0.0    0.0  239.0       0.0  0.0    0.0  179.0    0.0\n",
       " ⋮                                ⋮    ⋱                              ⋮\n",
       " 0.0  255.0    0.0  0.0   13.0    0.0       0.0  0.0    0.0    0.0    0.0\n",
       " 0.0  188.0    0.0  0.0   93.0    0.0       0.0  0.0    0.0    0.0    0.0\n",
       " 0.0   19.0   55.0  0.0  253.0    0.0  …    0.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  212.0  0.0  158.0    0.0     131.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  253.0  0.0    0.0    0.0     217.0  0.0   16.0    0.0    0.0\n",
       " 0.0    0.0  161.0  0.0    0.0    0.0     254.0  0.0  227.0    0.0    0.0\n",
       " 0.0    0.0   11.0  0.0    0.0    0.0     254.0  0.0  218.0    0.0    0.0\n",
       " 0.0    0.0   26.0  0.0    0.0  153.0  …  254.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  178.0  0.0    0.0  253.0     254.0  0.0    0.0    0.0    0.0\n",
       " 0.0    0.0  253.0  0.0    0.0  169.0     254.0  0.0    0.0  132.0    0.0\n",
       " 0.0    0.0  236.0  0.0    0.0  192.0     177.0  0.0    0.0  249.0    0.0\n",
       " 0.0    0.0  113.0  0.0    0.0  253.0      38.0  0.0    0.0  254.0    0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = reshape(train0, 100, 28*27) * -1\n",
    "b = reshape(train1, 100, 28*27) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# form the A matrix using product of each feature v_i with y_i = ±1\n",
    "A = vcat(a, b)\n",
    "β = 0.1; # choose regularization parameter manually for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200×756 Matrix{Float32}:\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0  …    -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0       -0.0  -0.0  -128.0    -0.0  -0.0\n",
       " -0.0   -0.0    -0.0  -0.0    -0.0       -0.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0   -18.0  -0.0    -0.0       -0.0  -0.0  -255.0    -0.0  -0.0\n",
       " -0.0   -0.0  -219.0  -0.0    -0.0       -0.0  -0.0  -128.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0    -0.0  …    -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0   -45.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -186.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -253.0     -255.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -253.0  -0.0  -253.0       -0.0  -0.0    -0.0    -0.0  -0.0\n",
       " -0.0   -0.0  -198.0  -0.0  -150.0  …    -0.0  -0.0  -128.0  -255.0  -0.0\n",
       " -0.0   -0.0  -182.0  -0.0   -27.0       -0.0  -0.0  -255.0  -255.0  -0.0\n",
       " -0.0   -0.0  -247.0  -0.0    -0.0       -0.0  -0.0  -128.0  -191.0  -0.0\n",
       "  ⋮                                 ⋱                                 ⋮\n",
       "  0.0  255.0     0.0   0.0    13.0        0.0   0.0     0.0     0.0   0.0\n",
       "  0.0  188.0     0.0   0.0    93.0        0.0   0.0     0.0     0.0   0.0\n",
       "  0.0   19.0    55.0   0.0   253.0  …     0.0   0.0     0.0     0.0   0.0\n",
       "  0.0    0.0   212.0   0.0   158.0      131.0   0.0     0.0     0.0   0.0\n",
       "  0.0    0.0   253.0   0.0     0.0      217.0   0.0    16.0     0.0   0.0\n",
       "  0.0    0.0   161.0   0.0     0.0      254.0   0.0   227.0     0.0   0.0\n",
       "  0.0    0.0    11.0   0.0     0.0      254.0   0.0   218.0     0.0   0.0\n",
       "  0.0    0.0    26.0   0.0     0.0  …   254.0   0.0     0.0     0.0   0.0\n",
       "  0.0    0.0   178.0   0.0     0.0      254.0   0.0     0.0     0.0   0.0\n",
       "  0.0    0.0   253.0   0.0     0.0      254.0   0.0     0.0   132.0   0.0\n",
       "  0.0    0.0   236.0   0.0     0.0      177.0   0.0     0.0   249.0   0.0\n",
       "  0.0    0.0   113.0   0.0     0.0       38.0   0.0     0.0   254.0   0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  \n",
    "# set up for logistic regression: pot and dpot are both scalar functions \n",
    "pot = (z) -> # logistic loss function (aka potential function)\n",
    "dpot = (z) -> # derivative of the potential function \n",
    "\n",
    "# cost is the f(x) function given above\n",
    "# both cost and grad are functions that take a vector as an input \n",
    "cost = (x) -> # overall cost function\n",
    "grad = (x) -> # gradient of logistic regression cost function \n",
    "L = # Lipshitz constant\n",
    "\n",
    "@show round(L, digits=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Nesterov's FGD (fast gradient descent) to learn the weights. \n",
    "See homework 6, problem 8 for help completing the function. \n",
    "The momentum step is the same as in that homework problem; \n",
    "the difference is that you must use the input argument `grad` to calculate the gradient \n",
    "instead of hard-coding the least squares gradient. \n",
    "You will then call `ngd` with the `grad` function above for the logistic regression gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  \n",
    "\n",
    "\"\"\" \n",
    "``x, out = ngd(grad, x0; nIters::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "\n",
    "Implementation of Nesterov's FGD (fast gradient descent),\n",
    "given the gradient of the cost function\n",
    "\n",
    "In: \n",
    "- grad is a function that takes in x and calculates the gradient of the cost function with respect to x\n",
    "- x0 is an initial point\n",
    "Optional: \n",
    "- nIters is the number of iterations\n",
    "- L is the Lipschitz constant of the derivative of the cost function \n",
    "- fun is a function to evaluate every iteration \n",
    "\n",
    "Out: (x, out) \n",
    "- x is the guess of the minimizer after running nIters iterations \n",
    "- out is an Array of evaluations of the fun function \n",
    "\"\"\" \n",
    "function ngd(grad, x0; niter::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "    # these lines initialize the output array to have the correct size/type \n",
    "    fun_x0 = fun(x0, 0)\n",
    "    out = similar(Array{typeof(fun_x0)}, niter+1)\n",
    "    out[1] = fun_x0\n",
    "    \n",
    "    # set up some variables \n",
    "    \n",
    "    # run the FGD for niter iterations\n",
    "    for iter=1:niter\n",
    "        # you need to update t, x, and z here \n",
    "        out[iter+1] = fun(x, iter) # compute user-defined function (typically cost function) each iteration\n",
    "    end\n",
    "    \n",
    "    return x, out \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include this just for comparison \n",
    "function gd(grad, x0; niter::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "    # these lines initialize the output array to have the correct size/type \n",
    "    fun_x0 = fun(x0, 0)\n",
    "    out = similar(Array{typeof(fun_x0)}, niter+1)\n",
    "    out[1] = fun_x0\n",
    "    \n",
    "    # run GD for niter iterations\n",
    "    x = x0\n",
    "    for iter=1:niter\n",
    "        x = x - grad(x) / L\n",
    "        out[iter+1] = fun(x, iter) # compute cost each iteration\n",
    "    end\n",
    "    \n",
    "    return x, out \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your NGD code to the autograder, test it using the two blocks below. If your code is working, the cost function should descend to 39.33\n",
    "Include the plot of the cost function in your report and comment about whether or not FGD has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the previously set up gradient and cost function to call the lsngd function \n",
    "x0 = zeros(nx*ny)\n",
    "niter = 100\n",
    "x_fgd, cost_fgd = ngd(grad, x0, niter = niter, L=L, fun = (x, iter) -> cost(x));\n",
    "\n",
    "@show round(cost_fgd[end], digits=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(1:niter, cost_fgd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"FGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For comparison, also plot the cost function for standard (not fast) GD\n",
    "_, cost_gd = gd(grad, x0, niter = niter, L=L, fun = (x, iter) -> cost(x));\n",
    "scatter(1:niter, cost_gd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"GD\")\n",
    "scatter!(1:niter, cost_fgd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"FGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savefig(\"task6_part1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test the LR classifier \n",
    "+ The above test code ran 100 iterations of FGD. Now, display the final iterate xk for k = 100 by reshaping it into a 2D image.\n",
    "Include a picture of the weights in your report. Think about features of the weights and if they make sense (you do not need to include this in your report).  \n",
    "+ Use the logistic regression weights to classify the test data. \n",
    "Report the classification accuracy in your document.\n",
    "+ Make histograms of the inner products $<\\hat{x}, v_m>$ for the test data using the $\\hat{x}$ from the  logistic regression method, to see how well separated the two distributions\n",
    "(for 5 and for 8) are. Include the histograms in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine classifier weights\n",
    "jim(reshape(x_fgd, nx, ny))\n",
    "title!(\"Logistic regression weights\")\n",
    "#savefig(\"task6_part2_weights.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression based classification\n",
    "log_correct0 = sum(sign.(reshape(test0, nx*ny, :)' * x_fgd) .== 1)\n",
    "@show round(log_correct0 / ntest, digits=4)\n",
    "log_correct1 = sum(sign.(reshape(test1, nx*ny, :)' * x_fgd) .== -1)\n",
    "@show round(log_correct1 / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of inner products - logistic\n",
    "histogram(reshape(test0, nx*ny, :)' * x_fgd, title = \"Logistic\", xlabel = \"Inner Product\", label = \"5\", alpha = 0.7, nbins = 30)\n",
    "histogram!(reshape(test1, nx*ny, :)' * x_fgd, label = \"8\", alpha = 0.7, nbins = 30)\n",
    "#savefig(\"task6_part2_histLR.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Subspace based classification\n",
    "This next part of the notebook is the subspace based classification method that you've previously coded. Now we apply that method to the digits 5 and 8. To make sure your data is properly set-up, first run all the cells under Task 1 and report the classification accuracies in your document when using three basis vectors for both classes. If everything is working properly, both accuracies should be between 86% and 92%. Include these accuracies in your report. \n",
    "\n",
    "Note, you do not have to modify any code since this task is replicating code from a previous assignment, but now applying it to the digits 5 and 8.\n",
    "\n",
    "Next, use the scree plot to pick the number of basis vectors to use for your classifier and also report this classification accuracy in your report. There are many possible reasonable values - we are looking for you to describe how you pick one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use training data to estimate the subspaces\n",
    "u0, s0, _ = svd(reshape(train0, nx*ny, :))\n",
    "u1, s1, _ = svd(reshape(train1, nx*ny, :))\n",
    "\n",
    "r0, r1 = 3, 3 # start with three basis vectors \n",
    "q0 = reshape(u0[:,1:r0], nx, ny, :)\n",
    "q1 = reshape(u1[:,1:r1], nx, ny, :);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images of the 1st 3 components of first digit subspace\n",
    "# you do not need to include this image in your report \n",
    "jim(cat(q0[:,:,1:3],q1[:,:,1:3],dims=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classify all the test data based on your subspace estimates\n",
    "# and count number of misclassified digits\n",
    "Q0 = reshape(q0, nx*ny, r0)\n",
    "Q1 = reshape(q1, nx*ny, r1)\n",
    "\n",
    "y0 = reshape(test0, nx*ny, :)\n",
    "y00 = y0 - Q0*(Q0'*y0)\n",
    "y01 = y0 - Q1*(Q1'*y0)\n",
    "correct0 = (mapslices(norm, y00, dims = 1) .< mapslices(norm, y01, dims = 1))[:]\n",
    "@show round(sum(correct0) / ntest, digits=4)\n",
    "\n",
    "y1 = reshape(test1, nx*ny, :)\n",
    "y10 = y1 - Q0*(Q0'*y1)\n",
    "y11 = y1 - Q1*(Q1'*y1)\n",
    "correct1 = (mapslices(norm, y10, dims = 1) .> mapslices(norm, y11, dims = 1))[:]\n",
    "@show round(sum(correct1) / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some bad cases \n",
    "bad1 = findall(correct1 .== false)\n",
    "bad0 = findall(correct0 .== false)\n",
    "jim(cat(test1[:,:,bad1[1:2]], test0[:,:,bad0[1:2]], dims=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the scree plots to choose a rank!\n",
    "plot(1:ntrain, s0, line=(:dots, :blue), label=\"digit 5\")\n",
    "plot!(1:ntrain, s1, line=(:dots,:red), label=\"digit 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick the number of basis to use for classification based on the scree plot\n",
    "\n",
    "r0, r1 = ,  # TODO \n",
    "q0 = reshape(u0[:,1:r0], nx, ny, :)\n",
    "q1 = reshape(u1[:,1:r1], nx, ny, :);\n",
    "\n",
    "# re-run the classification test code to get the new test accuracies \n",
    "# report your choice of r0 and r1 along with the new accuracies in your report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: least-squares based classifier \n",
    "This part of the notebook is the least-squared based classification method that you've previously coded. Now we apply that method to the digits 5 and 8. Run all the cells under Part 4 and report the classification accuracies (for the test digits) in your document. If everything is working properly, both accuracies should be between 77% and 89%.\n",
    "\n",
    "Note, you do not have to modify any code since this task is replicating code from a previous assignment, but now applying it to the digits 5 and 8.\n",
    "\n",
    "In your report, comment on how the histogram for the LS classifier and the histogram for the logistic regression-based classifier relate to the classification accuracies for the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-based classifier\n",
    "A = cat(reshape(train0, :, ntrain)',  -1 .*reshape(train1, :, ntrain)', dims = 1)\n",
    "b = [ones(ntrain); ones(ntrain)] \n",
    "x_ls = A \\ b\n",
    "jim(reshape(x_ls, nx, ny)) # look at the LS regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the classification accuracy based on the LS coefficients 'x_ls'\n",
    "ls_correct0 = sum(sign.(reshape(test0, nx*ny, :)' * x_ls) .== 1)\n",
    "@show round(ls_correct0 / ntest, digits=4)\n",
    "ls_correct1 = sum(sign.(reshape(test1, nx*ny, :)' * x_ls) .== -1)\n",
    "@show round(ls_correct1 / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of inner products - LS\n",
    "histogram(reshape(test0, nx*ny, :)' * x_ls, title = \"LS\", xlabel = \"Inner Product\", label = \"5\", alpha = 0.7, nbins = 30)\n",
    "histogram!(reshape(test1, nx*ny, :)' * x_ls, label = \"8\", alpha = 0.7, nbins = 30)\n",
    "#savefig(\"task6_part4_histLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your report, explain why this block of code gives 100% classification accuracy \n",
    "# hint: examine size() of various components\n",
    "tmp0 = sum(sign.(reshape(train0, nx*ny, :)' * x_ls) .== 1)\n",
    "@show tmp0 / ntrain\n",
    "tmp1 = sum(sign.(reshape(train1, nx*ny, :)' * x_ls) .== -1)\n",
    "@show tmp1 / ntrain;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: try using Tikhonov regularization to see if you can improve the classification accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Nearest neighbor classification by angle\n",
    "The last part of the notebook considers classification by nearest neighbor, using the angle between vectors as the measure of distance. Include the classification accuracy in your report. Also think about why we might not want to use this classifier in a real application, even if it performs well (you do not need to write this up for your report). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getangle = (v1, v2) -> acos(v1'*v2/(norm(v1)*norm(v2))) # this is equivalent \n",
    "\n",
    "\"\"\"\n",
    "class = classify(test, train0, train1)\n",
    "\n",
    "Binary classification based on the minumum angle between the test vector and each vector in the training data. \n",
    "\n",
    "In: \n",
    "- test     :    Vector of length M. M is the number of features. \n",
    "- train0   :    M x N0 matrix. N0 is the number of training samples of class 0.\n",
    "- train1   :    M x N1 matrix. N1 is the number of training samples of class 1.  \n",
    "\n",
    "Out:\n",
    "- The class (0 or 1) for every vector in test. \n",
    "\"\"\"\n",
    "function classify_angle(test, train0, train1)\n",
    "    θ0min = minimum([getangle(test, train0[:,n]) for n=1:size(train0,2)])\n",
    "    θ1min = minimum([getangle(test, train1[:,n]) for n=1:size(train1,2)])\n",
    "\n",
    "    return θ0min < θ1min ? 0 : 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0r = reshape(train0, :, ntrain) # reshape the data so that each column is a vector with the data form one image \n",
    "train1r = reshape(train1, :, ntrain)\n",
    "test0r = reshape(test0, :, ntest)\n",
    "test1r = reshape(test1, :, ntest)\n",
    "\n",
    "correct0 = [classify_angle(test0r[:,n], train0r, train1r) == 0 for n = 1:ntest]\n",
    "correct1 = [classify_angle(test1r[:,n], train0r, train1r) == 1 for n = 1:ntest]\n",
    "\n",
    "@show round(sum(correct0) / ntest, digits=4)\n",
    "@show round(sum(correct1) / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extensions\n",
    "+ Investigate different regularization parameters β for logistic regression. Do not use all of the test data for tuning that parameter,\n",
    "otherwise you are essentially training with the test data! Instead, use, say, 100 samples for tuning β, and\n",
    "then test with the remaining “untouched” 800 images.\n",
    "+ Use this web tool to capture your own “hand-written” digit:\n",
    "http://web.eecs.umich.edu/~fessler/course/551/r/digitdraw.htm and save it as a JPG file. (On\n",
    "my Mac this worked on Chrome, try another browser if your first try fails.)\n",
    "Read that image into Julia (or Matlab) and down-size it to be the appropriate size for your digit\n",
    "classifier. Apply your classifier and see if it identifies the correct digit.\n",
    "+ Extend the notebook to multi-class classification (up to all 10 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
