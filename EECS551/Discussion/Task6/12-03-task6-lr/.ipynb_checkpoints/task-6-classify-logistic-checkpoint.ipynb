{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore logistic regression classification of 5,8 digits   \n",
    "2017-11-27 Jeff Fessler, University of Michigan <br>\n",
    "2018-11-27 Steven Whitaker - Convert to Julia v1.0 <br>\n",
    "2019-11-25 Caroline Crockett - Add comments and sections, function template for NGD <br>\n",
    "2020-11-19 Caroline Crockett - Fix NGD template and change order of the parts <br>\n",
    "2021-11-23 Zongyu Li - Add `MLDatasets` and switch to Julia v1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Formation and General Instructions\n",
    "You may work individually, but we recommend that you work in pairs or groups of three. Find someone to work with and introduce yourself to them! One of you should copy the Google Document template and share the document with edit permissions with your group member(s): \n",
    "https://docs.google.com/document/d/1jw2209GnhUDYXsnoKfolb-HPoznJleDtFedsk-jb_hA/template/preview (you must use your umich email address to access this google document). \n",
    "\n",
    "\n",
    "The Google Document will include your answers to a couple of questions that will be asked. For each question, pick someone to type up your **group's** response. After completing the response, the group member should sign their name by typing it below the answer. For the next question, have a different group member respond and sign their name in the same way. Keep rotating until all the questions have been answered.\n",
    "\n",
    "The goal of this group exercise is to formulate your response as a group to the problem. When finished, **one** student in the group must submit a PDF of the google document to gradescope, entering the uniquenames of **all** students in the group. One group submits exactly one PDF, but we expect the PDF from different groups to differ. The deadline for submitting to gradescope will be announced on Canvas. Only submit the filled-in google document; do not submit a download of this Jupyter notebook. \n",
    "\n",
    "### Overview\n",
    "This task sheet develops logisitic regression based classification for classifying handwritten digits and then compares all the classification methods we have looked at this semester: nearest-angle-based classification (task 1), least-squares based classification (task 2), subspace-based classification (task 3 and 5), and logistic regression classification. \n",
    "\n",
    "We focus on classifying digits “5” and “8” because those are more difficult to distinguish and benefit from more advanced classification methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots: plot, plot!, scatter, scatter!, savefig, title!, histogram, histogram!\n",
    "using MIRTjim: jim\n",
    "using MLDatasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Download and shape data\n",
    "- x0, train0, and test0 contain data for digit \"5\"\n",
    "- x1, train1, and test1 contain data for digit \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the MNIST data file for digits 5 and 8\n",
    "# download from web if needed\n",
    "\n",
    "if !@isdefined(data)\n",
    "    digitn = [5, 8]\n",
    "    tmpx, tmpy = MNIST.traindata(Float32)\n",
    "    data = n -> tmpx[:,:,findall(tmpy .== n)[1:1000]] # 1st 1000 of digit n\n",
    "    data = 255 * cat(dims=4, data.(digitn)...)\n",
    "    nx, ny, nrep, ndigit = size(data)\n",
    "    data = data[:,2:ny,:,:] # make images non-square to force debug\n",
    "    ny = size(data,2)\n",
    "    @show size(data)\n",
    "end\n",
    "\n",
    "x0 = data[:,:,:,1]\n",
    "x1 = data[:,:,:,2]\n",
    "\n",
    "@show size(x0);\n",
    "@show size(x1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a couple of the images\n",
    "jim(cat(x0[:,:,44:45], x1[:,:,654:655], dims = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some data for training, and some for test\n",
    "ntrain = 100\n",
    "ntest = nrep - ntrain\n",
    "train0 = x0[:,:,1:ntrain] # training data\n",
    "train1 = x1[:,:,1:ntrain]\n",
    "test0 = x0[:,:,(ntrain+1):end] # testing data\n",
    "test1 = x1[:,:,(ntrain+1):end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "This part is the core part of the logistic regression classifier.\n",
    "+ Following the course notes, form a training data matrix A that has 200 rows (one row per training data sample), where the rows\n",
    "are multiplied by ±1 based on using a class label of -1 for digit 5 and class label +1 for digit 8 \n",
    "+ Normalize each row of A to have unit norm.\n",
    "+ Use the formula in the lecture notes to compute the Lipschitz constant for the logistic regression cost\n",
    "function gradient for β = 0.1. Record the value of L in your report.\n",
    "+ Implement Nesterov’s fast gradient descent (FGD).\n",
    "+ Apply FGD to the logistic regression cost function. In your report, include the plot of the\n",
    "cost function versus iteration and discuss whether FGD has converged. \n",
    "\n",
    "The logistic regression cost function is: <br>\n",
    "$f(x) = \\sum_i h(y_i ⟨x,v_i⟩) + \\frac{β}{2} \\lVert x \\rVert_2^2$ <br>\n",
    "$f(x) = 1_M' h.(Ax)         + \\frac{β}{2} \\lVert x \\rVert_2^2 $\n",
    "\n",
    "Where $h(z) = log(1+e^{-z})$ and $A[i,:] = y_i v_i'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# form the A matrix using product of each feature v_i with y_i = ±1\n",
    "A = # use train0 and train1 variables to form A \n",
    "β = 0.1; # choose regularization parameter manually for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  \n",
    "# set up for logistic regression: pot and dpot are both scalar functions \n",
    "pot = (z) -> # logistic loss function (aka potential function)\n",
    "dpot = (z) -> # derivative of the potential function \n",
    "\n",
    "# cost is the f(x) function given above\n",
    "# both cost and grad are functions that take a vector as an input \n",
    "cost = (x) -> # overall cost function\n",
    "grad = (x) -> # gradient of logistic regression cost function \n",
    "L = # Lipshitz constant\n",
    "\n",
    "@show round(L, digits=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Nesterov's FGD (fast gradient descent) to learn the weights. \n",
    "See homework 6, problem 8 for help completing the function. \n",
    "The momentum step is the same as in that homework problem; \n",
    "the difference is that you must use the input argument `grad` to calculate the gradient \n",
    "instead of hard-coding the least squares gradient. \n",
    "You will then call `ngd` with the `grad` function above for the logistic regression gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  \n",
    "\n",
    "\"\"\" \n",
    "``x, out = ngd(grad, x0; nIters::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "\n",
    "Implementation of Nesterov's FGD (fast gradient descent),\n",
    "given the gradient of the cost function\n",
    "\n",
    "In: \n",
    "- grad is a function that takes in x and calculates the gradient of the cost function with respect to x\n",
    "- x0 is an initial point\n",
    "Optional: \n",
    "- nIters is the number of iterations\n",
    "- L is the Lipschitz constant of the derivative of the cost function \n",
    "- fun is a function to evaluate every iteration \n",
    "\n",
    "Out: (x, out) \n",
    "- x is the guess of the minimizer after running nIters iterations \n",
    "- out is an Array of evaluations of the fun function \n",
    "\"\"\" \n",
    "function ngd(grad, x0; niter::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "    # these lines initialize the output array to have the correct size/type \n",
    "    fun_x0 = fun(x0, 0)\n",
    "    out = similar(Array{typeof(fun_x0)}, niter+1)\n",
    "    out[1] = fun_x0\n",
    "    \n",
    "    # set up some variables \n",
    "    \n",
    "    # run the FGD for niter iterations\n",
    "    for iter=1:niter\n",
    "        # you need to update t, x, and z here \n",
    "        out[iter+1] = fun(x, iter) # compute user-defined function (typically cost function) each iteration\n",
    "    end\n",
    "    \n",
    "    return x, out \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include this just for comparison \n",
    "function gd(grad, x0; niter::Int = 200, L::Real = 0, fun = (x, iter) -> 0)\n",
    "    # these lines initialize the output array to have the correct size/type \n",
    "    fun_x0 = fun(x0, 0)\n",
    "    out = similar(Array{typeof(fun_x0)}, niter+1)\n",
    "    out[1] = fun_x0\n",
    "    \n",
    "    # run GD for niter iterations\n",
    "    x = x0\n",
    "    for iter=1:niter\n",
    "        x = x - grad(x) / L\n",
    "        out[iter+1] = fun(x, iter) # compute cost each iteration\n",
    "    end\n",
    "    \n",
    "    return x, out \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your NGD code to the autograder, test it using the two blocks below. If your code is working, the cost function should descend to 39.33\n",
    "Include the plot of the cost function in your report and comment about whether or not FGD has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the previously set up gradient and cost function to call the lsngd function \n",
    "x0 = zeros(nx*ny)\n",
    "niter = 100\n",
    "x_fgd, cost_fgd = ngd(grad, x0, niter = niter, L=L, fun = (x, iter) -> cost(x));\n",
    "\n",
    "@show round(cost_fgd[end], digits=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(1:niter, cost_fgd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"FGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For comparison, also plot the cost function for standard (not fast) GD\n",
    "_, cost_gd = gd(grad, x0, niter = niter, L=L, fun = (x, iter) -> cost(x));\n",
    "scatter(1:niter, cost_gd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"GD\")\n",
    "scatter!(1:niter, cost_fgd, xlabel=\"Iteration\", ylabel=\"Cost\", label=\"FGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savefig(\"task6_part1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test the LR classifier \n",
    "+ The above test code ran 100 iterations of FGD. Now, display the final iterate xk for k = 100 by reshaping it into a 2D image.\n",
    "Include a picture of the weights in your report. Think about features of the weights and if they make sense (you do not need to include this in your report).  \n",
    "+ Use the logistic regression weights to classify the test data. \n",
    "Report the classification accuracy in your document.\n",
    "+ Make histograms of the inner products $<\\hat{x}, v_m>$ for the test data using the $\\hat{x}$ from the  logistic regression method, to see how well separated the two distributions\n",
    "(for 5 and for 8) are. Include the histograms in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine classifier weights\n",
    "jim(reshape(x_fgd, nx, ny))\n",
    "title!(\"Logistic regression weights\")\n",
    "#savefig(\"task6_part2_weights.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression based classification\n",
    "log_correct0 = sum(sign.(reshape(test0, nx*ny, :)' * x_fgd) .== 1)\n",
    "@show round(log_correct0 / ntest, digits=4)\n",
    "log_correct1 = sum(sign.(reshape(test1, nx*ny, :)' * x_fgd) .== -1)\n",
    "@show round(log_correct1 / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of inner products - logistic\n",
    "histogram(reshape(test0, nx*ny, :)' * x_fgd, title = \"Logistic\", xlabel = \"Inner Product\", label = \"5\", alpha = 0.7, nbins = 30)\n",
    "histogram!(reshape(test1, nx*ny, :)' * x_fgd, label = \"8\", alpha = 0.7, nbins = 30)\n",
    "#savefig(\"task6_part2_histLR.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Subspace based classification\n",
    "This next part of the notebook is the subspace based classification method that you've previously coded. Now we apply that method to the digits 5 and 8. To make sure your data is properly set-up, first run all the cells under Task 1 and report the classification accuracies in your document when using three basis vectors for both classes. If everything is working properly, both accuracies should be between 86% and 92%. Include these accuracies in your report. \n",
    "\n",
    "Note, you do not have to modify any code since this task is replicating code from a previous assignment, but now applying it to the digits 5 and 8.\n",
    "\n",
    "Next, use the scree plot to pick the number of basis vectors to use for your classifier and also report this classification accuracy in your report. There are many possible reasonable values - we are looking for you to describe how you pick one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use training data to estimate the subspaces\n",
    "u0, s0, _ = svd(reshape(train0, nx*ny, :))\n",
    "u1, s1, _ = svd(reshape(train1, nx*ny, :))\n",
    "\n",
    "r0, r1 = 3, 3 # start with three basis vectors \n",
    "q0 = reshape(u0[:,1:r0], nx, ny, :)\n",
    "q1 = reshape(u1[:,1:r1], nx, ny, :);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images of the 1st 3 components of first digit subspace\n",
    "# you do not need to include this image in your report \n",
    "jim(cat(q0[:,:,1:3],q1[:,:,1:3],dims=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classify all the test data based on your subspace estimates\n",
    "# and count number of misclassified digits\n",
    "Q0 = reshape(q0, nx*ny, r0)\n",
    "Q1 = reshape(q1, nx*ny, r1)\n",
    "\n",
    "y0 = reshape(test0, nx*ny, :)\n",
    "y00 = y0 - Q0*(Q0'*y0)\n",
    "y01 = y0 - Q1*(Q1'*y0)\n",
    "correct0 = (mapslices(norm, y00, dims = 1) .< mapslices(norm, y01, dims = 1))[:]\n",
    "@show round(sum(correct0) / ntest, digits=4)\n",
    "\n",
    "y1 = reshape(test1, nx*ny, :)\n",
    "y10 = y1 - Q0*(Q0'*y1)\n",
    "y11 = y1 - Q1*(Q1'*y1)\n",
    "correct1 = (mapslices(norm, y10, dims = 1) .> mapslices(norm, y11, dims = 1))[:]\n",
    "@show round(sum(correct1) / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some bad cases \n",
    "bad1 = findall(correct1 .== false)\n",
    "bad0 = findall(correct0 .== false)\n",
    "jim(cat(test1[:,:,bad1[1:2]], test0[:,:,bad0[1:2]], dims=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the scree plots to choose a rank!\n",
    "plot(1:ntrain, s0, line=(:dots, :blue), label=\"digit 5\")\n",
    "plot!(1:ntrain, s1, line=(:dots,:red), label=\"digit 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick the number of basis to use for classification based on the scree plot\n",
    "\n",
    "r0, r1 = ,  # TODO \n",
    "q0 = reshape(u0[:,1:r0], nx, ny, :)\n",
    "q1 = reshape(u1[:,1:r1], nx, ny, :);\n",
    "\n",
    "# re-run the classification test code to get the new test accuracies \n",
    "# report your choice of r0 and r1 along with the new accuracies in your report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: least-squares based classifier \n",
    "This part of the notebook is the least-squared based classification method that you've previously coded. Now we apply that method to the digits 5 and 8. Run all the cells under Part 4 and report the classification accuracies (for the test digits) in your document. If everything is working properly, both accuracies should be between 77% and 89%.\n",
    "\n",
    "Note, you do not have to modify any code since this task is replicating code from a previous assignment, but now applying it to the digits 5 and 8.\n",
    "\n",
    "In your report, comment on how the histogram for the LS classifier and the histogram for the logistic regression-based classifier relate to the classification accuracies for the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-based classifier\n",
    "A = cat(reshape(train0, :, ntrain)',  -1 .*reshape(train1, :, ntrain)', dims = 1)\n",
    "b = [ones(ntrain); ones(ntrain)] \n",
    "x_ls = A \\ b\n",
    "jim(reshape(x_ls, nx, ny)) # look at the LS regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the classification accuracy based on the LS coefficients 'x_ls'\n",
    "ls_correct0 = sum(sign.(reshape(test0, nx*ny, :)' * x_ls) .== 1)\n",
    "@show round(ls_correct0 / ntest, digits=4)\n",
    "ls_correct1 = sum(sign.(reshape(test1, nx*ny, :)' * x_ls) .== -1)\n",
    "@show round(ls_correct1 / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of inner products - LS\n",
    "histogram(reshape(test0, nx*ny, :)' * x_ls, title = \"LS\", xlabel = \"Inner Product\", label = \"5\", alpha = 0.7, nbins = 30)\n",
    "histogram!(reshape(test1, nx*ny, :)' * x_ls, label = \"8\", alpha = 0.7, nbins = 30)\n",
    "#savefig(\"task6_part4_histLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your report, explain why this block of code gives 100% classification accuracy \n",
    "# hint: examine size() of various components\n",
    "tmp0 = sum(sign.(reshape(train0, nx*ny, :)' * x_ls) .== 1)\n",
    "@show tmp0 / ntrain\n",
    "tmp1 = sum(sign.(reshape(train1, nx*ny, :)' * x_ls) .== -1)\n",
    "@show tmp1 / ntrain;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: try using Tikhonov regularization to see if you can improve the classification accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Nearest neighbor classification by angle\n",
    "The last part of the notebook considers classification by nearest neighbor, using the angle between vectors as the measure of distance. Include the classification accuracy in your report. Also think about why we might not want to use this classifier in a real application, even if it performs well (you do not need to write this up for your report). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getangle = (v1, v2) -> acos(v1'*v2/(norm(v1)*norm(v2))) # this is equivalent \n",
    "\n",
    "\"\"\"\n",
    "class = classify(test, train0, train1)\n",
    "\n",
    "Binary classification based on the minumum angle between the test vector and each vector in the training data. \n",
    "\n",
    "In: \n",
    "- test     :    Vector of length M. M is the number of features. \n",
    "- train0   :    M x N0 matrix. N0 is the number of training samples of class 0.\n",
    "- train1   :    M x N1 matrix. N1 is the number of training samples of class 1.  \n",
    "\n",
    "Out:\n",
    "- The class (0 or 1) for every vector in test. \n",
    "\"\"\"\n",
    "function classify_angle(test, train0, train1)\n",
    "    θ0min = minimum([getangle(test, train0[:,n]) for n=1:size(train0,2)])\n",
    "    θ1min = minimum([getangle(test, train1[:,n]) for n=1:size(train1,2)])\n",
    "\n",
    "    return θ0min < θ1min ? 0 : 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0r = reshape(train0, :, ntrain) # reshape the data so that each column is a vector with the data form one image \n",
    "train1r = reshape(train1, :, ntrain)\n",
    "test0r = reshape(test0, :, ntest)\n",
    "test1r = reshape(test1, :, ntest)\n",
    "\n",
    "correct0 = [classify_angle(test0r[:,n], train0r, train1r) == 0 for n = 1:ntest]\n",
    "correct1 = [classify_angle(test1r[:,n], train0r, train1r) == 1 for n = 1:ntest]\n",
    "\n",
    "@show round(sum(correct0) / ntest, digits=4)\n",
    "@show round(sum(correct1) / ntest, digits=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extensions\n",
    "+ Investigate different regularization parameters β for logistic regression. Do not use all of the test data for tuning that parameter,\n",
    "otherwise you are essentially training with the test data! Instead, use, say, 100 samples for tuning β, and\n",
    "then test with the remaining “untouched” 800 images.\n",
    "+ Use this web tool to capture your own “hand-written” digit:\n",
    "http://web.eecs.umich.edu/~fessler/course/551/r/digitdraw.htm and save it as a JPG file. (On\n",
    "my Mac this worked on Chrome, try another browser if your first try fails.)\n",
    "Read that image into Julia (or Matlab) and down-size it to be the appropriate size for your digit\n",
    "classifier. Apply your classifier and see if it identifies the correct digit.\n",
    "+ Extend the notebook to multi-class classification (up to all 10 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
